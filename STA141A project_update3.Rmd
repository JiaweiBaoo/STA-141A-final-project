---
title: "STA141A project"
output: html_document
date: "2024-03-18"
---

```{r setup, echo = F, message = FALSE}
library(plyr)
library(stats)
library(ggplot2)
library(dplyr)
library(MASS)
library(knitr)
library(pheatmap)
library(gridExtra)
library(caret)
#library(tidyverse)
library(ROCR)

opts_chunk$set(fig.cap="",
               fig.pos = "H",
               #out.extra = "",
               dpi=1500,
               warning = FALSE, 
               message = F,
               echo = F)
set.seed(1)
```


# A logsitic preditive model for perceptual decision made by mic using visual stimuli and neuronal activities 

# Abstract

Previous studies have shown that the decision-making process of a mouse can be affected by visual stimuli and neuronal activities. In this manuscript, we analyzed part of the experimental data from the experiments conducted by Steinmetz et al. (2019) to build a predictive model to predict a mouse action using visual stimuli and neuronal activities. 

# Section 1 Introduction. 

Studies have shown that a perceptual decision made by mice is determined by the visual stimuli and neuronal activities that may be distributed across different brain areas (Steinmetz et al., 2019). While neuronal activities happen almost across the whole brain, neurons encoding visual stimuli and upcoming actions only exist in several restricted regions of the brain. In addition, the strength and direction of visual stimuli also play a role in the decision-making process while stronger contrast between two visual stimuli is more likely to cause a "correct" action. Therefore, it is reasonable to hypothesize that a perceptual decision/action can be predicted with specific visual stimuli and neuronal activities of the mouse brain.

In the study conducted by Steinmetz et al. (2019), experiments were conducted on multiple mice over multiple sessions, which contained multiple trials. For each trial, a visual stimulus (left or right contrast) was given to the mouse and neuronal activities were recorded during the time it made the perceptual decision to either move right or left. The objective of this project is to build a predictive model that can be used to predict the perceptual decision/action made by the mouse using the neural activity data.


# Section 2 Exploratory analysis. 

```{r}
## load data 
session=list()
for(i in 1:18){
  session[[i]]=readRDS(paste('./Data/session',i,'.rds',sep=''))
  # print(session[[i]]$mouse_name)
  # print(session[[i]]$date_exp)
}
```

From Table 1, we can see that a total of four mice were tested in 18 sessions. A total of 62 brain areas were test in the experiment. The number of neurons involved ranged from 474 (session 16) to 1769 (session 4). The number of trials also varied among different sessions, ranging from 114 (session 1) to 447 (session 10). The distribution of stimulus conditions seemed to be roughly equally separated into 3 categories: left, right, and equal (i.e., left equals right), for most sessions. The feedback of a mouse is considered to be a "success" in the following three situation: (1) the mouse turns the wheel to the right when a right stimulus occurs; (2) the mouse turns the wheel to the left when a left stimulus occurs; (3) the mouse turns wheel to either direction (left or right) when a equal stimulus occurs. Otherwise,the feedback is considered to be a "failure". The mean success feedback was $70.8%$ across all trials in 18 sessions. The mouse Lederberg seemed to have a higher percentage of Success feedback than other mice. 

We tested if there is a significant difference in feedback across sessions by a $\chi ^2$ test. Since the p-value is larger than 0.05 (p-value =0.235), we concluded that there is no significant differences in the probability of success feedback across different sessions. This suggests a consistent decision-making process across sessions. We then test if there is a significant difference in feedback across mice. Since the p-value is larger than 0.05 (p-value =0.36), we concluded that there is no significant differences in the probability of success feedback across different mice. This suggests a consistent decision-making process across mice. 

```{r}
neutrons = lapply(session, function(s) {
  num = dim(s$spks[[1]])[1]
  return(num)
}) %>% unlist()

trials = lapply(session, function(s) {
  num = length(s$contrast_left)
  return(num)
}) %>% unlist()


stimuli_condition = laply(session, function(s){
  left = mean(s$contrast_right > s$contrast_left)*100 ## percentage of left > right
  equal = mean(s$contrast_right == s$contrast_left)*100
  return(c(left ,  100-left-equal, equal ))
}) %>% unlist()

feedback = laply(session, function(s){
 mean(s$feedback_type== 1)*100 ## percentage of correct choice
})

mouse = lapply(session, function(s) s$mouse_name) %>% unlist()


tb1 = data.frame(mouse = mouse, neutrons= neutrons,trials=trials) 
tb1 = cbind(tb1, stimuli_condition,feedback)
colnames(tb1) = c("Mouse","Neutrons numbers", "Trial numbers", 
                  "Stimuli condition: Left (%)","Stimuli condition: Right (%)",
                  "Stimuli condition: Equal (%)", "Success feedback (%)")
tb1.new = cbind(Session = 1:18 %>% as.character(), tb1)
kable(tb1.new, caption = "Describe the data structures across sessions")


```

```{r include=F}
### overall success feedback 
mean(tb1$`Success feedback (%)`)
```


```{r include=F}
# brain area
area = c()
for(i in 1:length(session)){
    tmp = session[[i]];
    area = c(area, unique(tmp$brain_area))
}

area = unique(area)
length(area)
```


```{r include=F}

chisq.test(tb1.new$`Success feedback (%)`, tb1.new$Session )
```

```{r}
chisq.test(tb1.new$Mouse, tb1.new$`Success feedback (%)`)
```


We created a new variable, `lr`, to represent the difference between right and left contrast at each trail, and lr = right - left. Figure 1 shows that there are no significant differences in the distribution of the difference between right and left contrast across sessions. 

```{r fig.cap="Figure 1: Boxplot of the difference between right and left contrast for each session", out.width="70%"}
## box plot of left contrast

stimuli = lapply(session, function(s) {
  left = s$contrast_left
  right = s$contrast_right
  return(data.frame(left = left, right = right))
})

## combine the stimuli from all sessions into one dataframe
stimuli_combined = do.call(rbind, lapply(seq_along(stimuli), function(i) {
  stimuli[[i]]$Session = as.character(i)
  return(stimuli[[i]])
}))

stimuli_combined = stimuli_combined[c(ncol(stimuli_combined), 1:(ncol(stimuli_combined)-1))]
## create a new variable, lr to represent the difference between right and left contrast
## lr = right - left

stimuli_combined = stimuli_combined %>% 
                  mutate(lr = right - left) %>% 
                  mutate(lr.abs = abs(lr), 
                         left.contrast = ifelse(lr<0, 1, 0)) ## if left>right

ggplot(data = stimuli_combined, aes(y = lr,group = Session, fill = Session))+
  geom_boxplot()+ 
  theme_minimal()


```

To quantify the activity level of neurons, we calculated the firing rate, which is the total number of spikes a neuron emits divided by the duration of observation time, for each neuron at each trial. We then obtained the average firing rate for the same brain area for each trial in each session. Figure 2 contains the heat maps of the average firing rate for different Brain area in each experiment Session. The color scale indicates the value of the average firing rate, where the darker shades of purple represent higher average firing rate, while lighter shades of pink represent lower average firing rate. The row labels indicate the different trials and the column labels represent the Brain area (e.g., ACA). We can see that the value of average firing rate of the same Brain area is generally consistent across all trials in the same experiment session as the shades along the same column label stayed relatively stable. In other words, within the same session, the distribution of firing rate tended to be consistent across trials. However, the distribution firing rate varies among different sessions. In addition, the brain areas experimented on in different sessions were quite different.

```{r include=F}

plot_list = list()

for(i in 1:18){
  firing_rate = lapply(session[[i]]$spks, function(s){
    fir = rowMeans(s)
    res= data.frame(firing_rate = fir, Brain_erea =session[[i]]$brain_area )
    return(res)
  })
  firing_rate_combined = do.call(rbind, lapply(seq_along(firing_rate), function(j) {
  firing_rate[[j]]$Trial = j
  return(firing_rate[[j]])
          }))
  
firing_rate_combined.wide = firing_rate_combined %>% 
                            dplyr::group_by(Brain_erea, Trial) %>% 
                            summarise(firing_rate = mean(firing_rate)) %>% 
                            ungroup() #%>% 
                           # tidyr::pivot_wider(names_from = Brain_erea, values_from = firing_rate) %>% 
                            #dplyr::select(-Trial)
#rownames(firing_rate_combined.wide) = paste0("Trial", 1:length(session[[i]]$feedback_type))

#pheatmap(firing_rate_combined.wide)

plot_list[[i]] =ggplot(firing_rate_combined.wide, aes(x=Brain_erea, y=Trial, fill=firing_rate)) +
  geom_tile() +
  scale_fill_gradient(low="pink", high="blue") +
  theme_minimal()+
  theme(plot.title = element_text(size = 8),
        axis.title.x = element_text(size = 6),
        axis.title.y = element_text(size = 6),
        axis.text.x = element_text(size = 5),
        legend.text = element_text(size = 5),
        legend.title = element_text(size = 6)) +
  labs(fill="Firing rate", x="Brain area", y="Trial", title=paste0("Session", i)) 

}

```

```{r out.width="70%"}
do.call(gridExtra::grid.arrange, c(plot_list[c(1:6)], ncol = 2))
do.call(gridExtra::grid.arrange, c(plot_list[c(7:12)], ncol = 2))
```


```{r fig.cap = "Figure 2: HeatMaps of Firing rate across 18 sessions",out.width="70%"}


do.call(gridExtra::grid.arrange, c(plot_list[c(13:18)], ncol = 2))

```


# Section 3 Data integration. 

We performed a Principal Component Analysis (PCA) to identify common neural activity across sessions. The PCA results suggested that the first 9 principal components (PCs) cumulatively explained about $90\%$ variation in the response variable (`feedback`). Therefore, we decided to use the first 9 PCs to build the predictive model.

The left plot in Figure 3 displays the first two PCs (PC1 and PC2) of the sample data with each point representing one trial across all 18 sessions and different colors representing different sessions. The x-axis represents PC1, which accounts for the largest variance in the data, and the y-axis represents PC2, which accounts for the second-largest variance. Each data point represents a specific firing rate, and its position on the plot reflects its values for PC1 and PC2. The combination of color and shape of the points represent of the experiment session (e.g., black circle represents data points from Session 1). We can see that data from the same session tends to be clustered together and all the sessions except for Session 13 are also very close to each other in the left plot in Figure 3. The separation of Session 13 from the rest suggests that data from Session 13 might be significantly different from the rest in terms of PC scores. Similarly, the right plot in Figure 3 displays the PC3 and PC4 of the sample data. The x-axis represents PC3, which accounts for the third-largest variance in the data, and the y-axis represents PC4, which accounts for the fourth-largest variance. Similar to what we observed from the left plot, data points from the same session tends to be clustered together. Data points from Sessions 3 and 12 are separated from data from the rest Sessions, which suggests that Sessions 3 and 12 might be significantly different from the rest in terms of PC scores. Therefore, we decided to combine data from all sessions except for Session 3, 12, and 13. Instead, we created three new binary variables to indicate whether the data were coming from Session 3, 12, and 13, respectively.

Since the strength and direction of visual stimuli also play a role in the decision-making process while stronger contrast between two visual stimuli, we created a new variable which is the absolute difference between the right and left contrast. We also created a new variable that indicates whether stimuli from the left are stronger than the right. We then combine all the variables into one data set to build a predictive model.

```{r include=F}
fr.com = list()
 for(i in 1:18){
  firing_rate = lapply(session[[i]]$spks, function(s){
    fir = rowMeans(s)
    res= data.frame(firing_rate = fir, Brain_erea =session[[i]]$brain_area )
    return(res)
  })
 firing_rate_combined = do.call(rbind, lapply(seq_along(firing_rate), function(j) {
  firing_rate[[j]]$Session = i
  firing_rate[[j]]$Trial = j
  return(firing_rate[[j]])
          }))
  
fr.com[[i]] = firing_rate_combined %>% 
                            dplyr::group_by(Brain_erea, Trial, Session) %>% 
                            summarise(firing_rate = mean(firing_rate)) %>% 
                            ungroup() %>% 
                            tidyr::pivot_wider(names_from = Brain_erea, values_from = firing_rate) %>% 
                            as.data.frame()

 }

## combine results from different sessions into one dataframe
firing_rate_combined = dplyr::bind_rows(fr.com)

# # Normalize data (calculate z-scores)
# fr.normalized = firing_rate_mean_combined %>%
#             group_by(Brain_erea,Session) %>%
#             mutate(fr.normalized = scale(firing_rate)) %>% 
#             as.data.frame()

## convert NA values to 0
firing_rate_combined[is.na(firing_rate_combined)] = 0

# fr.normalized.wide = fr.normalized %>% 
#                   group_by(Session, Brain_erea) %>%
#                   #summarise(fr.ave = mean(fr.normalized)) %>% 
#                   #ungroup() %>% 
#                   group_by(Session) %>%
#                   tidyr::pivot_wider(names_from = Brain_erea, values_from = fr.ave )
# 
# 

firing.pca <- firing_rate_combined[,-c(1,2)] %>% prcomp(center = F, scale = F)
#plot(x=firing.pca$x[,1],y=firing.pca$x[,2], pch=16,xlab="PC 1", ylab="PC 2")

print(summary(firing.pca))


```


```{r out.width="70%", fig.cap="Figure 3: PCA plot of Firing rate across 18 sessions"}
par(mfrow=c(1,2))
plot(firing.pca$x[,1:2], col = as.factor(firing_rate_combined$Session),
     pch = firing_rate_combined$Session,
     main="PC1 v.s. PC2")
legend("bottomleft", legend = levels(as.factor(firing_rate_combined$Session)),
       col = 1:length(levels(as.factor(firing_rate_combined$Session))),
       pch = 1:length(levels(as.factor(firing_rate_combined$Session))), cex= 0.56)

plot(firing.pca$x[,3:4], col = as.factor(firing_rate_combined$Session),
     pch = firing_rate_combined$Session,
     main="PC3 v.s. PC4")
legend("bottomleft", legend = levels(as.factor(firing_rate_combined$Session)),
       col = 1:length(levels(as.factor(firing_rate_combined$Session))), 
       pch = 1:length(levels(as.factor(firing_rate_combined$Session))), cex= 0.56)
```








```{r eval=F}
#Calculate mean firing rate for each neuron across trials within a session to reduce dimensionality

firing_rate_mean = list()
for(i in 1:18){
  ## get the firing rate for each trial 
  firing_rate = lapply(session[[i]]$spks, function(s){
    fir = rowMeans(s)
    
    return(fir)
  })
  
  n.trials = length(session[[i]]$feedback_type) ## number of trials 
  ## get mean firing rate for each neuron across trials
  firing_rate_mean[[i]] = data.frame(firing_rate = Reduce("+", firing_rate)/n.trials, 
                                     Brain_erea =session[[i]]$brain_area )
}

## combine results from different sessions into one dataframe
firing_rate_mean_combined = do.call(rbind, lapply(seq_along(firing_rate_mean), function(i) {
  firing_rate_mean[[i]]$Session = as.character(i)
  firing_rate_mean[[i]]$Mouse = session[[i]]$mouse_name
  return(firing_rate_mean[[i]])
}))


# Normalize data (calculate z-scores)
fr.normalized = firing_rate_mean_combined %>%
            group_by(Brain_erea, Mouse, Session) %>%
            mutate(fr.normalized = scale(firing_rate)) %>% 
            as.data.frame()
```


```{r}


## combine the feedback from all sessions into one dataframe
feedback_combined = lapply(session, function(s) {
  return(s$feedback_type)
}) %>% unlist()



## select firing data for predictive model
## select the first 9 pcs since they cumulatively explained 90.65% variance
firing.model = firing.pca$x[,1:9] %>% 
               as.data.frame() %>% 
               mutate(Session = firing_rate_combined$Session,
                      Session3 = ifelse(firing_rate_combined$Session==3, 1, 0),
                      Session12 = ifelse(firing_rate_combined$Session==12, 1, 0),
                      Session13 = ifelse(firing_rate_combined$Session==13, 1, 0),
                      feedback = ifelse(feedback_combined ==1, 1, 0), ## re-code it to be 0 or 1
                      #Trial = firing_rate_combined$Trial,
                      Mouse = firing_rate_combined$Mouse,
                      left_contrast = stimuli_combined$left.contrast,## if left stimuli > right 
                      lr.abs = stimuli_combined$lr.abs) ## abs difference between left and right

firing.model.final = firing.model %>% 
                     dplyr::select(-c(Session))

```


# Section 4 Predictive modeling. 

In our study, we aim to predict a mouse's reaction to a visual stimulus. Given the binary nature of the response variable (success or failure), we chose the logistic regression model as our final prediction model due to its interpretability, simplicity, and efficiency. We hypothesize that a mouse's action can be predicted based on particular visual stimuli and neuronal activities in certain brain areas. To address the potential overfitting issue of the logistic regression model, instead of including all 62 brain areas studied across multiple experimental sessions, we performed PCA on the brain area data. PCA allowed us to reduce the dimensionality of the data by identifying a smaller set of PCs that account for most of the variation in the brain area data. This approach helped identify the most important and informative features in the brain area data. Moreover, PCA provided guidance on how to better integrate data from different experimental sessions. We combined data with similar features and created dummy variables to indicate sessions with different data characteristics. This approach enabled us to utilize both common and unique information provided by each experimental session in our predictive model.

The final dataset used to build the predictive model consists of the following variables:

- PC1 to PC9: The first 9 Principal Components from the PCA of brain areas.
- Session 3, Session 12, Session 13: Dummy variables indicating whether the data comes from the respective experimental sessions.
- Feedback: The response variable indicating a success (feedback=1) or failure (feedback=0) action.
- Left_contrast: Indicates whether the stimuli from the left is stronger than the right.
- lr.abs: The absolute difference between the right and left contrast, created to account for the effect of the scale of stimulus contrast on mouse reactions.

We included the first 9 PCs (PC1 to PC9) in the model because they collectively explained over 90% of the variation in the brain area data. The inclusion of dummy variables for Sessions 3, 12, and 13 was based on the PCA results, which suggested that the data points from these three sessions differed from the rest of the sessions in terms of PC scores. We randomly, with a random seed of 123, split the original data into two part: Train and Test. The split ratio is $80 \%$ for the training set and $20 \%$ for the testing set, which is a typical ratio used in machine learning and statistical analysis. 


A logistic regression model was built using all the aforementioned variables in the sample data. The final model is displayed in Table 2.


```{r include=F}

# split
set.seed(123) # for reproducibility
n = nrow(firing.model.final)
train_size = floor(0.8 * n)
train_indices = sample(1:n, train_size)
train_data = firing.model.final[train_indices, ]
test_data = firing.model.final[-train_indices, ]

model1 = glm(feedback~ ., data = train_data, family = "binomial")
summary(model1)
```

```{r}
res = summary(model1)
model.tb = res$coefficients
kable(model.tb, caption = "Predictive model")
```


# Section 5 Prediction performance on the test sets. 

By leveraging logistic regression and PCA, we developed a predictive model to forecast a mouse's reaction to a particular visual stimuli. The combination of dimensionality reduction through PCA and the interpretability of logistic regression allowed us to identify the most informative features and build a parsimonious model. The inclusion of session-specific dummy variables and the `lr.abs` variable further enhanced the model's ability to capture the nuances in the data. This approach demonstrates the power of integrating statistical techniques to unravel complex relationships in neuroscientific data and predict behavioral outcomes.

The fitted probability of success from the predictive model is overwhelmingly larger than 0.5 for most trials, which suggests that the proposed predictive model is more likely to predict a successful outcome. Therefore, we decided to set the threshold of success or failure to 0.6 to correct this bias and make more precise predictions. In other words, we classified a prediction with a predictive probability larger than 0.6 as success (i.e., 1) and otherwise as a failure (i.e., 0). We created a confusion matrix to summarize the performance of the proposed predictive model by comparing the predicted outcome with actual outcome. It provides a comprehensive view of how well the model correctly classified instances and where it made mistakes. From the confusion matrix, we can see that the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) are 628, 49, 248, and 92, respectively. The overall accuracy of the model, calculated as ( $\mathrm{TP}+\mathrm{TN}) /(\mathrm{TP}+\mathrm{TN}+\mathrm{F}$ FN), is $0.6657$, indicating that the model correctly classified $66.57 \%$ of the mice's reaction. 

The specificity of the model is $0.1650$, meaning that $16.50\%$ of the failure (i.e., 0) response were correctly predicted. The sensitivity of the proposed model is $0.8722$, indicating that the model corrected predicted $87.22/%$ of the success response (i.e., 1). The p-value of the hypothesis test on whether the model's accuracy is equal to the No Information Rate, which is the accuracy that would be achieved by always predicting the majority class, is $0.9985$. This suggested strong evidence of the model's accuracy is not significantly better than the No Information Rate. However, the result of the Mcnemar's Test sugguested that the proposed model's performance is significantly different from random guessing (p-value $<0.0001$). 

```{r include=F}
mean(model1$fitted.values>0.5)
mean(firing.model.final$feedback==0, na.rm=T)
```


```{r}
threshold = 0.6
prediction = predict(model1, newdata = test_data, type="response")
prediction = ifelse(prediction<threshold, 0, 1)
prediction.factor = factor(prediction, levels = c(1,0),
                           labels = c("Success", "Failure"))
Actual = factor(test_data$feedback,levels = c(1,0),
                           labels = c("Success", "Failure"))

CM = confusionMatrix(prediction.factor, reference= Actual)
kable(CM$table, caption="Confusion Matrix of the predicted feedback and the actual feedback")
```


```{r}
CM
```

## Test the model's performance on 50 random trails from session 1
 
We created a confusion matrix to summarize the performance of the proposed predictive model by comparing the predicted outcome with actual outcome using 50 randomly selected data from Session 1. From the confusion matrix, we can see that the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) are 37, 0, 0, and 13, respectively. The overall accuracy of the model, calculated as ( $\mathrm{TP}+\mathrm{TN}) /(\mathrm{TP}+\mathrm{TN}+\mathrm{F}$ FN), is $0.74$, indicating that the model correctly classified $74 \%$ of the mice's reaction. 

The specificity of the model is $0$, meaning that $0\%$ of the failure (i.e., 0) response were correctly predicted. The sensitivity of the proposed model is $1$, indicating that the model corrected predicted $100%$ of the success response (i.e., 1). The p-value of the hypothesis test on whether the model's accuracy is equal to the No Information Rate, which is the accuracy that would be achieved by always predicting the majority class, is $0.57$. This suggested strong evidence of the model's accuracy is not significantly better than the No Information Rate. However, the result of the Mcnemar's Test sugguested that the proposed model's performance is significantly different from random guessing (p-value $<0.0001$). 

```{r}
########### With test data ####
## load test data 

Session.no = c(1,18)
test=list()
for(i in 1:2){
  test[[i]]=readRDS(paste('./test/test',i,'.rds',sep=''))

}


fr.com = list()
 for(i in 1:length(test)){
  firing_rate = lapply(test[[i]]$spks, function(s){
    fir = rowMeans(s)
    res= data.frame(firing_rate = fir, Brain_erea =test[[i]]$brain_area )
    return(res)
  })
 firing_rate_combined = do.call(rbind, lapply(seq_along(firing_rate), function(j) {
  firing_rate[[j]]$Session = Session.no[i] ## test1 = session 1, test2 = session 18
  firing_rate[[j]]$Trial = j
  return(firing_rate[[j]])
          }))
  
fr.com[[i]] = firing_rate_combined %>% 
                            group_by(Brain_erea, Trial, Session) %>% 
                            summarise(firing_rate = mean(firing_rate)) %>% 
                            ungroup() %>% 
                            tidyr::pivot_wider(names_from = Brain_erea, values_from = firing_rate) %>% 
                            as.data.frame()

 }

## combine results from different sessions into one dataframe
firing_rate_combined = dplyr::bind_rows(fr.com)

# # Normalize data (calculate z-scores)
# fr.normalized = firing_rate_mean_combined %>%
#             group_by(Brain_erea,Session) %>%
#             mutate(fr.normalized = scale(firing_rate)) %>% 
#             as.data.frame()

## convert NA values to 0
firing_rate_combined[is.na(firing_rate_combined)] = 0

# fr.normalized.wide = fr.normalized %>% 
#                   group_by(Session, Brain_erea) %>%
#                   #summarise(fr.ave = mean(fr.normalized)) %>% 
#                   #ungroup() %>% 
#                   group_by(Session) %>%
#                   tidyr::pivot_wider(names_from = Brain_erea, values_from = fr.ave )
# 
# 

firing.pca <- firing_rate_combined[,-c(1,2)] %>% prcomp(center = F, scale = F)
#plot(x=firing.pca$x[,1],y=firing.pca$x[,2], pch=16,xlab="PC 1", ylab="PC 2")

#print(summary(firing.pca))


## combine the feedback from all sessions into one dataframe
feedback_combined = lapply(test, function(s) {
  return(s$feedback_type)
}) %>% unlist()

stimuli = lapply(test, function(s) {
  left = s$contrast_left
  right = s$contrast_right
  return(data.frame(left = left, right = right))
})

## combine the stimuli from all sessions into one dataframe
stimuli_combined = do.call(rbind, lapply(seq_along(stimuli), function(i) {
  stimuli[[i]]$Session = as.character(i)
  return(stimuli[[i]])
}))

stimuli_combined = stimuli_combined[c(ncol(stimuli_combined), 1:(ncol(stimuli_combined)-1))]
## create a new variable, lr to represent the difference between right and left contrast
## lr = right - left

stimuli_combined = stimuli_combined %>% 
                  mutate(lr = right - left) %>% 
                  mutate(lr.abs = abs(lr), 
                         left.contrast = ifelse(lr<0, 1, 0)) ## if left>right


## select firing data for predictive model
## select the first 9 pcs since they cumulatively explained 90.65% variance
firing.model.test = firing.pca$x[,1:9] %>% 
               as.data.frame() %>% 
               mutate(Session = firing_rate_combined$Session,
                      Session3 = ifelse(firing_rate_combined$Session==3, 1, 0),
                      Session12 = ifelse(firing_rate_combined$Session==12, 1, 0),
                      Session13 = ifelse(firing_rate_combined$Session==13, 1, 0),
                      feedback = ifelse(feedback_combined ==1, 1, 0), ## re-code it to be 0 or 1
                      #Trial = firing_rate_combined$Trial,
                      Mouse = firing_rate_combined$Mouse,
                      left_contrast = stimuli_combined$left.contrast,## if left stimuli > right 
                      lr.abs = stimuli_combined$lr.abs) ## abs difference between left and right


```

```{r}

S1 = firing.model.test %>% 
    filter(Session==1)

# split
set.seed(123) # for reproducibility
n = nrow(S1)
test_size =50
test_indices = sample(1:n, test_size)
test_data = S1[test_indices, ]

threshold = 0.6
prediction = predict(model1, newdata = test_data, type="response")
prediction = ifelse(prediction<threshold, 0, 1)
prediction.factor = factor(prediction, levels = c(1,0),
                           labels = c("Success", "Failure"))
Actual = factor(test_data$feedback,levels = c(1,0),
                           labels = c("Success", "Failure"))

CM = confusionMatrix(prediction.factor, reference= Actual)
kable(CM$table, caption="Confusion Matrix of the predicted feedback for Session 1 and the actual feedback")
```

```{r}
CM
```

## Test the model's performance on 50 random trails from session 18

We created a confusion matrix to summarize the performance of the proposed predictive model by comparing the predicted outcome with actual outcome using 50 randomly selected data from Session 1. From the confusion matrix, we can see that the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) are 39, 0, 0, and 11, respectively. The overall accuracy of the model, calculated as ( $\mathrm{TP}+\mathrm{TN}) /(\mathrm{TP}+\mathrm{TN}+\mathrm{F}$ FN), is $0.78$, indicating that the model correctly classified $78 \%$ of the mice's reaction. 

The specificity of the model is $0$, meaning that $0\%$ of the failure (i.e., 0) response were correctly predicted. The sensitivity of the proposed model is $1$, indicating that the model corrected predicted $100%$ of the success response (i.e., 1). The p-value of the hypothesis test on whether the model's accuracy is equal to the No Information Rate, which is the accuracy that would be achieved by always predicting the majority class, is $0.58$. This suggested strong evidence of the model's accuracy is not significantly better than the No Information Rate. However, the result of the Mcnemar's Test sugguested that the proposed model's performance is significantly different from random guessing (p-value $=0.003$). 

```{r}

S18 = firing.model.test %>% 
    filter(Session==18)

# split
set.seed(123) # for reproducibility
n = nrow(S18)
test_size =50
test_indices = sample(1:n, test_size)
test_data = S18[test_indices, ]

threshold = 0.6
prediction = predict(model1, newdata = test_data, type="response")
prediction = ifelse(prediction<threshold, 0, 1)
prediction.factor = factor(prediction, levels = c(1,0),
                           labels = c("Success", "Failure"))
Actual = factor(test_data$feedback,levels = c(1,0),
                           labels = c("Success", "Failure"))

CM = confusionMatrix(prediction.factor, reference= Actual)
kable(CM$table, caption="Confusion Matrix of the predicted feedback for Session 18 and the actual feedback")
```


```{r}
CM
```

# Section 6 Discussion. 

Motivated by the previous studies on the perceptual decision made by mice, we built a logistic regression model to predict a action made by a mouse using visual stimuli and neuronal activities. The complexity of the neural encoding in the decision-making process is increased by the variance in neurons tested across different sessions and mice. We explored the neural activities during each trial and the variations across different sessions. Additionally, we investigated the probability of successful actions and visual stimuli across sessions. To extract shared patterns and address distinct sessions, we conducted a PCA. The logistic regression model achieved an accuracy of $67.39%$ on the test dataset, with a sensitivity of $0.8722$ and a specificity of $0.1650$ with respect to the test data. We have also tested the performance of the proposed predictive model with 50 randomly selected sample from Session 1 and Session 18, respectively. The results were similar to what we observed in the original test data. That is the proposed model performed very well in predicting Success while it performed poor in predicting Failure. 


While these metrics indicate reasonable performance, especially in predicting successful responses, there are several drawbacks and areas for improvement. First of all, there exist a bias towards successful outcomes in the proposed model. It seemed to preform poorly in predicting failure responses, which might be attributed to the predicted probability of success being overwhelmingly larger than 0.5 for most trials. This suggests that the model is biased towards predicting successful outcomes. To correct for this bias, we set the threshold for success or failure to 0.6 instead of 0.5. However, this adjustment might not completely eliminate the bias. To better address this bias, we could consider including more variables related to visual stimulus, as expanding the feature set with additional relevant information may help the model better distinguish between successful and failed responses. Secondly, the logistic regression model, while being well studied and interpretable, may not be able to capture complex relationships between visual stimuli, neuronal activities, and mouse actions. It is likely for the decision-making process in mice to involves non-linear relationships between the response and predictors that a simple logistic regression model may not adequately capture. In addition, the current model failed to account for the temporal aspects of neuronal activities and mouse actions, which plays an crucial role in the decision-making process. We can explore more advanced regression model, such as recurrent neural networks, which is able to capture more complex relationships between response and predictors and model the temporal dynamics of neuronal activities, for the future study. 






# Reference {-}

Steinmetz, N.A., Zatka-Haas, P., Carandini, M. et al. Distributed coding of choice, action and engagement across the mouse brain. Nature 576, 266–273 (2019). https://doi.org/10.1038/s41586-019-1787-x
